{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:06:56.317322Z",
     "start_time": "2023-01-22T03:06:55.620402Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:06:57.705692Z",
     "start_time": "2023-01-22T03:06:56.321164Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:06:57.825111Z",
     "start_time": "2023-01-22T03:06:57.711395Z"
    }
   },
   "outputs": [],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:54.265151Z",
     "start_time": "2023-01-22T03:33:53.915358Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:54.661091Z",
     "start_time": "2023-01-22T03:33:54.647892Z"
    }
   },
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:55.281124Z",
     "start_time": "2023-01-22T03:33:55.144585Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Convert the dataset to a dictionary\n",
    "data_dict = squad[\"train\"].to_dict()\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:56.165238Z",
     "start_time": "2023-01-22T03:33:55.601287Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:56.853380Z",
     "start_time": "2023-01-22T03:33:56.175262Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:56.869103Z",
     "start_time": "2023-01-22T03:33:56.864148Z"
    }
   },
   "outputs": [],
   "source": [
    "offset_mapping = inputs.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T03:33:57.669113Z",
     "start_time": "2023-01-22T03:33:57.443910Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers']\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:16:05.785112Z",
     "start_time": "2023-01-22T04:16:05.548343Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_train.csv',index=False)\n",
    "train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:16:48.893326Z",
     "start_time": "2023-01-22T04:16:48.818540Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Convert the dataset to a dictionary\n",
    "data_dict = squad[\"test\"].to_dict()\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:16:50.969386Z",
     "start_time": "2023-01-22T04:16:49.700485Z"
    }
   },
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:16:51.009113Z",
     "start_time": "2023-01-22T04:16:50.971945Z"
    }
   },
   "outputs": [],
   "source": [
    "offset_mapping = inputs.pop(\"offset_mapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:16:51.873168Z",
     "start_time": "2023-01-22T04:16:51.756542Z"
    }
   },
   "outputs": [],
   "source": [
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers']\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[\"answer_start\"][0]\n",
    "    end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:17:06.985276Z",
     "start_time": "2023-01-22T04:17:06.715695Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_test.csv',index=False)\n",
    "test = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:17:29.037138Z",
     "start_time": "2023-01-22T04:17:27.631538Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-22T04:19:10.789151Z",
     "start_time": "2023-01-22T04:17:39.064087Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_qa_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
